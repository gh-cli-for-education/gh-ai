#!/usr/bin/env node
/**
 * Universidad de La Laguna
 * Escuela Superior de Ingeniería y Tecnología
 * Grado en Ingeniería Informática
 * Trabajo de Fin de Grado
 *
 * @author Raimon José Mejías Hernández  <alu0101390161@ull.edu.es>
 * @date 12/02/2024
 * @desc @TODO hacer la descripción
 */
import OpenAI from 'openai';
import { sleep } from 'openai/core.js';

import { CONSOLE_PROMPT } from '../utils.js';
import { TOOLS } from './tools.js';
import readline from 'readline/promises';

'use strict';

const DEFAULT_MODEL = 'gpt-3.5-turbo-0125';
const READLINE = readline.createInterface({ input: process.stdin, output: process.stdout });

/**
 * 
 * @param {OpenAI} openai 
 * @param {string} systemPrompt 
 * @param {string} llmModel 
 * @param {object} toolDescriptions 
 * @param {string} assistantID 
 * @returns {object}
 */
async function createOrRetreiveAssistant(openai, systemPrompt, llmModel, toolDescriptions, assistantID) { 

  const ASSISTANT_CONFIGURATION = {
    model: llmModel ?? DEFAULT_MODEL,
    name: 'gh-ai-assistant',
    description: 'Assistant generated by the gh-ai extension',
    tools: toolDescriptions,
    instructions: systemPrompt
  };

  if (assistantID) { // Si en el process.env existe un assistant ID, se extrae dicho assistant en lugar de crear uno nuevo  

    let assistant = await openai.beta.assistants.retrieve(assistantID);
    let askForUpdate = false;

    const unwrapper = (({model, name, description, instructions}) => {
       return {model, name, description, instructions};
    });

    const ASSISTANT_OLD_CONFIGURATION = unwrapper(assistant);

    for (const KEY of Object.keys(ASSISTANT_OLD_CONFIGURATION)) { // Recorre el objeto comprobando que tengan la misma configuración

      /** 
       * @TODO Ver como hacer para que se pueda comprobar que contiene las tools correctas
       * Ver como implementar un tag que mantega el ultimo tipo de commandType que ejecuto este assistant,
       * con eso ya se puede saber si se ha cambiado gran parte de la configuración 
      */
      if (ASSISTANT_CONFIGURATION[KEY] !== ASSISTANT_OLD_CONFIGURATION[KEY]) { 

        console.log(`${CONSOLE_PROMPT.WARNING}The old assistant configuration of "${assistant.name}" doesn't match with the new configuration readed from the inputObject.`);
        askForUpdate = true;
        break; 

      }

    }

    // En caso de que alguna configuración no coincida, preguntar al usuario si quiere actualizarlo
    while (askForUpdate) {

      const response = await READLINE.question('  Do you want to update the assistant configuration?(Y[es]/N[o]): ');

      if (/^y(?:es)?$/i.exec(response)) {

        console.log(`${CONSOLE_PROMPT.GH_AI}Updating assistant's configuration.`);
        assistant = await openai.beta.assistants.update(assistant.id, ASSISTANT_CONFIGURATION);
        break;

      } 
      else if (/^n(?:o)?$/i.exec(response)) { break; }

    }

    return assistant;

  }

  // Si no existe process.env.ASSISTANT_ID se genera un nuevo asistente
  return await openai.beta.assistants.create(ASSISTANT_CONFIGURATION);

}

/**
 * 
 * @param {OpenAI} openai 
 * @param {string} threadID 
 */
async function createOrRetreiveThread(openai, threadID) {

  if (threadID) {

    return await openai.beta.threads.retrieve(threadID);

  } 

  return await openai.beta.threads.create();

}

/**
 * 
 * @param {OpenAI} openai 
 * @param {string} systemPrompt 
 * @param {*} options 
 * @returns 
 */
async function createAssistantAndThread(openai, systemPrompt, llmModel, tools) {
  const MODEL = llmModel || 'gpt-3.5-turbo-0125';

  let assistant;

  if (process.env.ASSISTANT_ID) {
    assistant = await openai.beta.assistants.retrieve(process.env.ASSISTANT_ID);
  } 
  else {
    assistant = await openai.beta.assistants.create({ model: MODEL });
  }
  
  // independientemente de si se ha creado o se ha utilizado uno anterior se actualiza sus datos para que use la información actual.
  assistant = await openai.beta.assistants.update(
    assistant.id,
    {
      name: 'gh-ai-assistant',
      model: MODEL,
      description: 'Assistant generated by the gh-ai extension',
      tools: tools,
      instructions: systemPrompt
    }
  );

  let thread;
  if (process.env.THREAD_ID) {
    thread = await openai.beta.threads.retrieve(process.env.THREAD_ID);
  } 
  else {
    thread = await openai.beta.threads.create();
  }

  return [assistant, thread];
}

/**
 * 
 * @param {OpenAI} openai 
 * @param {string} assistantID 
 * @param {string} threadID 
 * @param {string} outputDirectory 
 * @param {object} options 
 * @returns 
 */
async function call(openai, prompt, assistantID, threadID, outputDirectory, options) {

  // Añadir el mensaje a la conversación
  await openai.beta.threads.messages.create(
    threadID,
    {
      role: 'user',
      content: prompt
    }
  );

  const DELAY = 10000; // 10s
  const MAX_TRIES = 10;
  let currentTry = 0;
  
  let result = {};
  while (currentTry < MAX_TRIES) {
    result = await checkRunStatus(openai, assistantID, threadID, outputDirectory, options);

    if (result.completed) { return result; }

    if (result.error) {
      console.error(`${CONSOLE_PROMPT.ERROR}The API call couldn't be executed correctly, Generating Logs and stopping execution.`);
      return result;
    }

    currentTry++;
    await sleep(DELAY);
  }

  result.error = true;
  console.error(`${CONSOLE_PROMPT.ERROR}The program used the max amount of calls but the API didn't respond, finishing execution.`)
  return result;
}

// Ver como puedo quitar de aquí el outputDirectory
async function checkRunStatus(openai, assistantID, threadID, outputDirectory, options) {

  const DELAY = 2000; // ms
  let failure = false;

  // Empezar la conversación  
  let run = await openai.beta.threads.runs.create(
    threadID,
    { assistant_id: assistantID, }
  ); 

  // Comprobar constantemente si la conversación ha terminado correctamente
  while (!failure) {
    // Comprobar la conversación en el momento actual
    run = await openai.beta.threads.runs.retrieve(threadID, run.id);
  
    switch (run.status) {

      case 'failed':
        let result = { completed: false, error: false, run: run }
        if (run.last_error.code === 'rate_limit_exceeded') {
          console.log(`${CONSOLE_PROMPT.WARNING}Rate limit exceeded, applying a 10s delay.`);
        } 
        else {
          console.log(`${CONSOLE_PROMPT.ERROR}The run failed while attempting to talk with the AI.`);
          result.error = true;
        }
        return result;

      case 'expired':
        console.log(`${CONSOLE_PROMPT.ERROR}The run reached the 10 minutes limit.`); 
        return { completed: false, error: true, run: run };

      case 'queued':
        console.log(`${CONSOLE_PROMPT.OPENAI}The run is still in queue. Waiting for the API to received.`);
        break;
  
      case 'in_progress':
        console.log(`${CONSOLE_PROMPT.OPENAI}The run is still active. Waiting for the API to response.`);
        break;
  
      case 'cancelling':
        console.log(`${CONSOLE_PROMPT.OPENAI}The run is being cancelled.`);
        break;
  
      case 'cancelled':
        console.log(`${CONSOLE_PROMPT.OPENAI}The run has been cancelled successfully.`);
        return { completed: false, error: true, run: run };
  
      case 'requires_action':
        console.log(`${CONSOLE_PROMPT.OPENAI}The run requires an action from a tool. Waiting for the result.`);
        const TOOL_OUTPUTS = await manageToolActions(run, outputDirectory, options);
        run = await openai.beta.threads.runs.submitToolOutputs(
          threadID,
          run.id,
          TOOL_OUTPUTS
        );
        console.log(`${CONSOLE_PROMPT.OPENAI}Tool executed successfully!`);
        break;
  
      case 'completed':
        console.log(`${CONSOLE_PROMPT.OPENAI}The run has been completed, extracting the AI response.`);
        return { completed: true, error: false, run: run };
          
      default: 
        throw new OpenAI.APIError(-1, {
          type: 'no_status_support_error', 
          message: 'The run is in a not supported status.'
        });
    }

    await sleep(DELAY);
  }
}

/**
 * 
 * @param {*} run 
 * @param {*} outputDirectory 
 * @param {*} options 
 * @returns 
 */
async function manageToolActions(run, outputDirectory, options) {
  if (run.required_action.type === 'submit_tool_outputs') {

    let requiredActions = run.required_action.submit_tool_outputs;
    let outputs = await Promise.all(requiredActions.tool_calls.map(async (call) => {

      console.log(`\tDEBUG>: Executing ${call.function.name} tool.`);
      return {
        tool_call_id: call.id,
        output: await TOOLS[call.function.name](call.function.arguments, outputDirectory, options)
      }

    }));
    return { tool_outputs: outputs };

  }

  throw new OpenAI.APIError(-3, {
    type: 'submit_tool_outputs_error',
    message: 'The action required was not a tool output submition.'
  });
}

export { createOrRetreiveAssistant, createOrRetreiveThread, call };