#!/usr/bin/env node
/**
 * Universidad de La Laguna
 * Escuela Superior de Ingeniería y Tecnología
 * Grado en Ingeniería Informática
 * Trabajo de Fin de Grado
 *
 * @author Raimon José Mejías Hernández  <alu0101390161@ull.edu.es>
 * @date 12/02/2024
 * @desc @TODO hacer la descripción
 */
import OpenAI from 'openai';
import { sleep } from 'openai/core.js';

import { API } from './utils.js';
import { COLORS } from './colors.js';
import { TOOLS, TOOLS_DESCRIPTIONS } from './openai-api-tools.js';
'use strict';

const GH_AI_PROMPT  = COLORS.yellow('GH-AI>: ');
const OPENAI_PROMPT = COLORS.blue(`GH-AI-OPENAI>: `);
const ERROR_PROMPT  = COLORS.red(`GH-AI-ERROR>: `);
const WARNING_PROMPT  = COLORS.magenta(`GH-AI-WARNING>: `);

/**
 * 
 * @param {*} promptObject 
 * @param {*} outputDirectory 
 * @param {*} options 
 * @returns 
 */
API['OPENAI'] = async function(promptObject, outputDirectory, options) {
  
  const OPENAI = new OpenAI({ 
    apiKey: process.env.OPENAI_API_KEY,
    organization: process.env.OPENAI_ORG
  });

  let responseObject = {
    systemPrompt: promptObject.system,
    messages: [],
    usage: {
      totalPromptTokens: 0,
      totalCompletionTokens: 0,
      totalTokens: 0,
    },
    readme: {
      prompt: promptObject.readme,
      result: undefined 
    },
    config: undefined,
    assistant: undefined,
    thread: undefined,
  };

  let assistant = undefined;
  let thread = undefined;

  try {
    [assistant, thread] = await createAssistantAndThread(OPENAI, promptObject.system, options);
    
    if (options.saveAssistant) { responseObject.assistant = assistant.id; }
    if (options.saveThread) { responseObject.thread = thread.id; }

    for (const PROMPT of promptObject.user) {
      console.log(`${GH_AI_PROMPT}Working with ${PROMPT.title} prompt`);
      // console.log(PROMPT);

      for (const PROMPT_SECTION of PROMPT.content) {
        // Llamar a la API y añadir la respuesta al reponseObject
        const CALL_RESULT = await call(OPENAI, PROMPT_SECTION, assistant.id, thread.id, outputDirectory, options);
        await addResultToResponseObject(OPENAI, CALL_RESULT, responseObject, { title: PROMPT.title, content: PROMPT_SECTION }, thread.id, options);
        if (CALL_RESULT.error) { // En caso de error detener la ejecución
          return responseObject;
        }
      }
      
    }

    if (responseObject.readme.prompt) {

      console.log(`${GH_AI_PROMPT}Working with README.md prompt`);

      const CALL_RESULT = await call(OPENAI, responseObject.readme.prompt, assistant.id, thread.id, outputDirectory, options);

      if (CALL_RESULT.error) { // En caso de error detener la ejecución
        return responseObject;
      }

      let messages = await OPENAI.beta.threads.messages.list(thread.id);
      messages = messages.data.filter(message => message.role === 'assistant');

      promptObject.readme.result = messages[0].content.map((content) => { return content.text.value;});

    }

    console.log(`\n${GH_AI_PROMPT}The ${options.llmApi} API call has been executed successfully!`);

    /** @TODO No tiene en cuenta los tokens usados por las tools */
    if (options.tokensVerbose) { // Calcular el total de tokens usados.
      for (const MESSAGE of responseObject.messages) {
        responseObject.usage.totalPromptTokens += MESSAGE.usage.prompt_tokens;
        responseObject.usage.totalCompletionTokens += MESSAGE.usage.completion_tokens;
        responseObject.usage.totalTokens += MESSAGE.usage.total_tokens;
      }
    }

    return responseObject;
  } 
  finally {
    
    if (!options.saveAssistant && assistant) {
      await OPENAI.beta.assistants.del(assistant.id);
    }
  
    if (!options.saveThread && thread) {
      await OPENAI.beta.threads.del(thread?.id);
    }

  }
}

/**
 * 
 * @param {*} openai 
 * @param {*} systemPrompt 
 * @param {*} options 
 * @returns 
 */
async function createAssistantAndThread(openai, systemPrompt, options) {
  const MODEL = options.llmModel || 'gpt-3.5-turbo-0125';

  let assistant;

  if (process.env.ASSISTANT_ID) {
    assistant = await openai.beta.assistants.retrieve(process.env.ASSISTANT_ID);
  } 
  else {
    assistant = await openai.beta.assistants.create({ model: MODEL });
  }
  
  // independientemente de si se ha creado o se ha utilizado uno anterior se actualiza sus datos para que use la información actual.
  assistant = await openai.beta.assistants.update(
    assistant.id,
    {
      name: 'gh-ai-assistant',
      model: MODEL,
      description: 'Assistant generated by the gh-ai extension',
      tools: TOOLS_DESCRIPTIONS,
      instructions: systemPrompt
    }
  );

  let thread;
  if (process.env.THREAD_ID) {
    thread = await openai.beta.threads.retrieve(process.env.THREAD_ID);
    console.log(thread);
  } 
  else {
    thread = await openai.beta.threads.create();
  }

  return [assistant, thread];
}

/**
 * 
 * @param {*} openai 
 * @param {*} callResult 
 * @param {*} responseObject 
 * @param {*} threadID 
 * @param {*} options 
 */
async function addResultToResponseObject(openai, callResult, responseObject, prompt, threadID, options) {
  if (callResult.error) { // Se genera una respuesta de error y se termina la ejecución del programa
    responseObject.messages.push({
      title: prompt.title,
      prompt: prompt.content,
      response: callResult.run.last_error.message,
      usage: undefined
    });

    responseObject.usage.totalPromptTokens = callResult.run.usage.prompt_tokens;
    responseObject.usage.totalTokens = callResult.run.usage.total_tokens;
  }

  let messages = await openai.beta.threads.messages.list(threadID);
  messages = messages.data.filter(message => message.role === 'assistant');

  responseObject.messages.push({
    title: prompt.title,
    prompt: prompt.content,
    response: messages[0].content.map((content) => { return content.text.value;}),
    usage: (options.tokensVerbose)? result.run.usage : undefined,
  });
}

/**
 * 
 * @param {*} openai 
 * @param {*} assistantID 
 * @param {*} threadID 
 * @param {*} outputDirectory 
 * @param {*} options 
 * @returns 
 */
async function call(openai, prompt, assistantID, threadID, outputDirectory, options) {

  // Añadir el mensaje a la conversación
  await openai.beta.threads.messages.create(
    threadID,
    {
      role: 'user',
      content: prompt
    }
  );

  const DELAY = 10000; // 10s
  const MAX_TRIES = 10;
  let currentTry = 0;
  
  let result = {};
  while (currentTry < MAX_TRIES) {
    result = await checkRunStatus(openai, assistantID, threadID, outputDirectory, options);

    if (result.completed) { return result; }

    if (result.error) {
      console.log(`${ERROR_PROMPT}The API call couldn't be executed correctly, Generating Logs and stopping execution.`);
      return result;
    }

    currentTry++;
    await sleep(DELAY);
  }

  result.error = true;
  console.log(`${ERROR_PROMPT}The program used the max amount of calls but the API didn't respond, finishing execution.`)
  return result;
}

// Ver como puedo quitar de aquí el outputDirectory
async function checkRunStatus(openai, assistantID, threadID, outputDirectory, options) {

  const DELAY = 2000; // ms
  let failure = false;

  // Empezar la conversación  
  let run = await openai.beta.threads.runs.create(
    threadID,
    { assistant_id: assistantID, }
  ); 

  // Comprobar constantemente si la conversación ha terminado correctamente
  while (!failure) {
    // Comprobar la conversación en el momento actual
    run = await openai.beta.threads.runs.retrieve(threadID, run.id);
  
    switch (run.status) {

      case 'failed':
        let result = { completed: false, error: false, run: run }
        if (run.last_error.code === 'rate_limit_exceeded') {
          console.log(`${WARNING_PROMPT}Rate limit exceeded, applying a 10s delay.`);
        } 
        else {
          console.log(`${ERROR_PROMPT}The run failed while attempting to talk with the AI.`);
          result.error = true;
        }
        return result;

      case 'expired':
        console.log(`${ERROR_PROMPT}The run reached the 10 minutes limit.`); 
        return { completed: false, error: true, run: run };

      case 'queued':
        console.log(`${OPENAI_PROMPT}The run is still in queue. Waiting for the API to received.`);
        break;
  
      case 'in_progress':
        console.log(`${OPENAI_PROMPT}The run is still active. Waiting for the API to response.`);
        break;
  
      case 'cancelling':
        console.log(`${OPENAI_PROMPT}The run is being cancelled.`);
        break;
  
      case 'cancelled':
        console.log(`${OPENAI_PROMPT}The run has been cancelled successfully.`);
        return { completed: false, error: true, run: run };
  
      case 'requires_action':
        console.log(`${OPENAI_PROMPT}The run requires an action from a tool. Waiting for the result.`);
        const TOOL_OUTPUTS = await manageToolActions(run, outputDirectory, options);
        run = await openai.beta.threads.runs.submitToolOutputs(
          threadID,
          run.id,
          TOOL_OUTPUTS
        );
        console.log(`${OPENAI_PROMPT}Tool executed successfully!`);
        break;
  
      case 'completed':
        console.log(`${OPENAI_PROMPT}The run has been completed, extracting the AI response.`);
        return { completed: true, error: false, run: run };
          
      default: 
        throw new OpenAI.APIError(-1, {
          type: 'no_status_support_error', 
          message: 'The run is in a not supported status.'
        });
    }

    await sleep(DELAY);
  }
}

/**
 * 
 * @param {*} run 
 * @param {*} outputDirectory 
 * @param {*} options 
 * @returns 
 */
async function manageToolActions(run, outputDirectory, options) {
  if (run.required_action.type === 'submit_tool_outputs') {

    let requiredActions = run.required_action.submit_tool_outputs;
    let outputs = await Promise.all(requiredActions.tool_calls.map(async (call) => {

      console.log(`\tDEBUG>: Executing ${call.function.name} tool.`);
      return {
        tool_call_id: call.id,
        output: await TOOLS[call.function.name](call.function.arguments, outputDirectory, options)
      }

    }));
    return { tool_outputs: outputs };

  }

  throw new OpenAI.APIError(-3, {
    type: 'submit_tool_outputs_error',
    message: 'The action required was not a tool output submition.'
  });
}

export { API };