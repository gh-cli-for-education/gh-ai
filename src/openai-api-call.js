#!/usr/bin/env node
/**
 * Universidad de La Laguna
 * Escuela Superior de Ingeniería y Tecnología
 * Grado en Ingeniería Informática
 * Trabajo de Fin de Grado
 *
 * @author Raimon José Mejías Hernández  <alu0101390161@ull.edu.es>
 * @date 12/02/2024
 * @desc @TODO hacer la descripción
 */
import OpenAI from 'openai';
import { sleep } from 'openai/core.js';

import { API } from './utils.js';

import { TEMPLATES } from './templates/templates.js'; // update TEMPLATES object
import { COLORS } from './colors.js';
import { TOOLS, TOOLS_DESCRIPTIONS } from './openai-api-tools.js';
'use strict';

const GH_AI_PROMPT  = COLORS.yellow('GH-AI>: ');
const OPENAI_PROMPT = COLORS.blue(`GH-AI-OPENAI>: `);
const ERROR_PROMPT  = COLORS.red(`GH-AI-ERROR>: `);

/**
 * @description OpenAI specific implementation of the API call @TODO Mejorar esto
 * @param {object} prompts 
 * @param {*} options 
 * @returns 
 */
API['OPENAI'] = async function(inputObject, outputDirectory, options) {
  
  const OPENAI = new OpenAI({ 
    apiKey: process.env.OPENAI_API_KEY,
    organization: process.env.OPENAI_ORG
  });

  let response;
  let assistant;
  let thread;

  debugger;
  try {
    [assistant, thread] = await createAssistantAndThread(OPENAI, options);
    const TYPE = options.commandType.toUpperCase();

    const PROMPTS = {
      system: TEMPLATES[TYPE].SYSTEM(inputObject),
      user: TEMPLATES[TYPE].USER(inputObject),
    };

    response = await CALL[TYPE](
      OPENAI,
      assistant, 
      thread,
      PROMPTS,
      outputDirectory,
      options
    );

    console.log(`\n${GH_AI_PROMPT} The ${options.llmApi} API call has been executed successfully!`);

    return [PROMPTS, response];
  } finally {
    
    if (options.saveAssistant) {
      response.assistant = assistant.id;
      console.log(`${GH_AI_PROMPT} The assistant ID has been saved.`);
    } else {
      await OPENAI.beta.assistants.del(assistant.id);
    }
  
    if (options.saveThread) {
      response.thread = thread.id;
      console.log(`${GH_AI_PROMPT} The thread ID has been saved.`);
    } else {
      await OPENAI.beta.threads.del(thread.id);
    }

  }
}

/**
 * 
 * @param {*} openai 
 * @param {*} options 
 * @returns 
 */
async function createAssistantAndThread(openai, options) {
  const MODEL = options.llmModel || 'gpt-3.5-turbo-0125';

  let assistant;

  if (process.env.ASSISTANT_ID) {
    assistant = await openai.beta.assistants.retrieve(process.env.ASSISTANT_ID);
  } else {
    assistant = await openai.beta.assistants.create({ model: MODEL });
  }
  
  assistant = await openai.beta.assistants.update(
    assistant.id,
    {
      name: 'gh-ai-assistant',
      model: MODEL,
      description: 'Assistant generated by the gh-ai extension',
      tools: TOOLS_DESCRIPTIONS,
    }
  );

  let thread;
  if (process.env.THREAD_ID) {
    thread = await openai.beta.threads.retrieve(process.env.THREAD_ID);
  } else {
    thread = await openai.beta.threads.create();
  }

  return [assistant, thread];
}

/** */
const CALL = Object.create(null);

/**
 * 
 * @param {*} openai 
 * @param {*} inputObject 
 * @param {*} outputDirectory 
 * @param {*} options 
 * @returns 
 */
CALL['EXTENSION'] = async function(
  openai, 
  assistant, 
  thread, 
  prompts, 
  outputDirectory, 
  options
) {

  // Declare the response object
  let apiResponse = {
    response: [],
    usage: {},
    llm: options.llmApi
  };

  let runID = '';
  const SLEEP_TIME = 3600;

  for (const file of prompts.user) {

    console.log(`\n${OPENAI_PROMPT}Working with file: ${file.name}.`);

    // Crear el mensaje 
    await openai.beta.threads.messages.create(
      thread.id,
      {
        role: 'user',
        content: file
      }
    );

    // Empezar la conversación 
    let run = await openai.beta.threads.runs.create(
      thread.id,
      {
        assistant_id: assistant.id, 
        instructions: prompts.system
      }
    );
    runID = run.id;

    const COMPLETED = await checkRunStatus(
      openai, 
      thread.id,
      run.id, 
      outputDirectory, 
      options
    );

    if (!COMPLETED) {
      run = await openai.beta.threads.runs.retrieve(thread.id, run.id);
      throw new OpenAI.APIError(-2, {
        type: run.last_error.code,
        message: run.last_error.message,
      });
    }

    const MESSAGES = await openai.beta.threads.messages.list(thread.id);
    let response = MESSAGES.data.filter(message => message.role === 'assistant');

    apiResponse.response.push(...response[0].content.map((content) => {
      return content.text.value;
    }));
    await sleep(SLEEP_TIME);
  }

  if (options.tokensVerbose) {
    const RUN = await openai.beta.threads.runs.retrieve(
      thread.id,
      runID
    );
      apiResponse.usage = RUN.usage;
  }

  return apiResponse;
}

/**
 * 
 * @param {*} run 
 */
async function checkRunStatus(openai, threadID, runID, outputDirectory, options) {
  const SLEEP_TIME = 3600;

  let completed = false;
  let failure = false;

  while(!completed) {
    let run = await openai.beta.threads.runs.retrieve(threadID, runID);
    let message = '';
    switch(run.status) {
      case 'failed':
        message = 'The run failed while attempting to talk with the AI.';
        failure = true;
        break; 
      
      case 'expired':
        message = 'The run reached the 10 minutes limit.';
        failure = true;
        break;
      
      case 'queued':
        message = 'The run is still in queue. Waiting for the API to received.';
        break;

      case 'in_progress':
        message = 'The run is still active. Waiting for the API to response.';
        break;

      case 'cancelling':
        message = 'The run is being cancelled.';
        break;

      case 'cancelled':
        message = 'The run has been cancelled successfully.';
        failure = true;
        break;

      case 'requires_action':
        message = 'The run requires an action from a function. Waiting for the result.';
        const TOOL_OUTPUTS = await manageToolActions(run, outputDirectory, options);
        run = await openai.beta.threads.runs.submitToolOutputs(
          threadID,
          runID,
          TOOL_OUTPUTS
        );
        break;

      case 'completed':
        message = 'The run has been completed, extracting the AI response.';
        completed = true;
        break;
        
      default: 
        throw new OpenAI.APIError(-1, {
          type: 'no_status_support_error', 
          message: 'The run is in a not supported status.'
        });
    }

    if (failure) {
      console.error(`${ERROR_PROMPT}${message}`);
      return false; 
    }

    console.log(`${OPENAI_PROMPT}${message}`);
    await sleep(SLEEP_TIME);
  }

  return true;
}

/**
 * 
 * @param {*} run 
 * @param {*} outputDirectory 
 * @param {*} options 
 * @returns 
 */
async function manageToolActions(run, outputDirectory, options) {
  if (run.required_action.type === 'submit_tool_outputs') {

    let requiredActions = run.required_action.submit_tool_outputs;
    let outputs = await Promise.all(requiredActions.tool_calls.map(async (call) => {

      console.log(`\tExecuting ${call.function.name} tool.`);
      return {
        tool_call_id: call.id,
        output: await TOOLS[call.function.name](call.function.arguments, outputDirectory, options)
      }

    }));
    return { tool_outputs: outputs };

  }

  throw new OpenAI.APIError(-3, {
    type: 'submit_tool_outputs_error',
    message: 'The action required was not a tool output submition.'
  });
}

export { API };